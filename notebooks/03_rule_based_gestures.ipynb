{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ac353e8",
   "metadata": {},
   "source": [
    "# 03 ‚Äî Rule-Based Hand Gestures\n",
    "\n",
    "This notebook introduces a **rule-based gesture system** on top of MediaPipe\n",
    "hand landmarks.\n",
    "\n",
    "Goals:\n",
    "- Isolate **gesture detection rules** (conditions)\n",
    "- Isolate **actions** (what happens when a gesture is detected)\n",
    "- Make the system **easy to extend** (add rules / actions without touching the core loop)\n",
    "\n",
    "This is the foundation for:\n",
    "- Keyboard control\n",
    "- Gamepad control\n",
    "- Custom interactions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238c2c9d",
   "metadata": {},
   "source": [
    "### Imports\n",
    "\n",
    "Libraries required for:\n",
    "- Webcam access and visualization\n",
    "- MediaPipe Tasks API (Hands Landmarker)\n",
    "- Landmark geometry analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc64720",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import time\n",
    "import math\n",
    "\n",
    "import mediapipe as mp\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n",
    "\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29269c0",
   "metadata": {},
   "source": [
    "### Hand Landmarker setup\n",
    "\n",
    "Downloads the MediaPipe hand landmark model if needed  \n",
    "and initializes the Hands detector (Tasks API)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8c7bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = Path(\"hand_landmarker.task\")\n",
    "\n",
    "if not MODEL_PATH.exists():\n",
    "    !wget -O hand_landmarker.task https://storage.googleapis.com/mediapipe-models/hand_landmarker/hand_landmarker/float16/1/hand_landmarker.task\n",
    "else:\n",
    "    print(\"hand_landmarker.task already exists\")\n",
    "\n",
    "base_options = python.BaseOptions(\n",
    "    model_asset_path=str(MODEL_PATH)\n",
    ")\n",
    "\n",
    "options = vision.HandLandmarkerOptions(\n",
    "    base_options=base_options,\n",
    "    num_hands=2,\n",
    "    min_hand_detection_confidence=0.7,\n",
    "    min_hand_presence_confidence=0.7\n",
    ")\n",
    "\n",
    "detector = vision.HandLandmarker.create_from_options(options)\n",
    "\n",
    "print(\"MediaPipe Tasks (Hands) successfully initialized!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38c0e2a",
   "metadata": {},
   "source": [
    "### Utility Functions\n",
    "\n",
    "Helper functions used across gesture rules.\n",
    "They must remain **simple and reusable**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8b0b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_distance(p1, p2):\n",
    "    \"\"\"Euclidean distance between two normalized landmarks.\"\"\"\n",
    "    return math.sqrt((p1.x - p2.x)**2 + (p1.y - p2.y)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f348c39f",
   "metadata": {},
   "source": [
    "### Gesture Rules (Conditions Only)\n",
    "\n",
    "Each rule:\n",
    "- Takes hand landmarks as input\n",
    "- Returns True / False\n",
    "- Contains **no side effects**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d13ef70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_pinch(landmarks, threshold=0.05):\n",
    "    \"\"\"Thumb tip close to index tip.\"\"\"\n",
    "    return get_distance(landmarks[4], landmarks[8]) < threshold\n",
    "\n",
    "\n",
    "def is_open_hand(landmarks, threshold=0.08):\n",
    "    \"\"\"All fingertips far from palm center.\"\"\"\n",
    "    palm = landmarks[0]\n",
    "    fingertips = [4, 8, 12, 16, 20]\n",
    "    return all(get_distance(landmarks[t], palm) > threshold for t in fingertips)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca0f0ea",
   "metadata": {},
   "source": [
    "### Actions\n",
    "\n",
    "Actions are triggered when a gesture rule is satisfied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e21f07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def action_pinch(hand_label):\n",
    "    print(f\"ü§è Pinch detected on {hand_label} hand\")\n",
    "\n",
    "\n",
    "def action_open_hand(hand_label):\n",
    "    print(f\"‚úã Open hand detected on {hand_label} hand\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11965b0a",
   "metadata": {},
   "source": [
    "### Gesture ‚Üí Action Mapping\n",
    "\n",
    "This table links:\n",
    "- a gesture rule\n",
    "- to an action\n",
    "\n",
    "Adding a new gesture = **one line**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e05422",
   "metadata": {},
   "outputs": [],
   "source": [
    "GESTURE_RULES = [\n",
    "    (\"Pinch\", is_pinch, action_pinch),\n",
    "    (\"OpenHand\", is_open_hand, action_open_hand),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c6fe904",
   "metadata": {},
   "source": [
    "### Real-Time Camera Loop\n",
    "\n",
    "- Captures webcam frames\n",
    "- Runs hand detection\n",
    "- Applies gesture rules\n",
    "- Executes matching actions\n",
    "\n",
    "Press **q** to quit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019b5589",
   "metadata": {},
   "outputs": [],
   "source": [
    "HAND_CONNECTIONS = [\n",
    "    (0, 1), (1, 2), (2, 3), (3, 4),             # Thumb\n",
    "    (0, 5), (5, 6), (6, 7), (7, 8),             # Index finger\n",
    "    (9, 10), (10, 11), (11, 12),                # Middle finger\n",
    "    (13, 14), (14, 15), (15, 16),               # Ring finger\n",
    "    (0, 17), (17, 18), (18, 19), (19, 20),      # Pinky\n",
    "    (5, 9), (9, 13), (13, 17)                   # Palm\n",
    "]\n",
    "\n",
    "# Camera initialization\n",
    "cap = cv2.VideoCapture(0)\n",
    "if not cap.isOpened():\n",
    "    raise RuntimeError(\"Cannot open camera\")\n",
    "\n",
    "try:\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"Failed to read frame\")\n",
    "            break\n",
    "\n",
    "        # Mirror view for a more natural interaction\n",
    "        frame = cv2.flip(frame, 1)\n",
    "        h, w, _ = frame.shape\n",
    "\n",
    "        # Convert frame to RGB (required by MediaPipe)\n",
    "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        mp_image = mp.Image(\n",
    "            image_format=mp.ImageFormat.SRGB,\n",
    "            data=rgb_frame\n",
    "        )\n",
    "\n",
    "        # Hand landmark detection\n",
    "        result = detector.detect(mp_image)\n",
    "\n",
    "        if result.hand_landmarks:\n",
    "            for hand_idx, landmarks in enumerate(result.hand_landmarks):\n",
    "                \n",
    "                # 1. Draw hand connections (skeleton)\n",
    "                for start_idx, end_idx in HAND_CONNECTIONS:\n",
    "                    start_pt = (\n",
    "                        int(landmarks[start_idx].x * w),\n",
    "                        int(landmarks[start_idx].y * h)\n",
    "                    )\n",
    "                    end_pt = (\n",
    "                        int(landmarks[end_idx].x * w),\n",
    "                        int(landmarks[end_idx].y * h)\n",
    "                    )\n",
    "                    cv2.line(frame, start_pt, end_pt, (0, 255, 0), 2)\n",
    "                \n",
    "                # 2. Draw landmark points\n",
    "                for lm in landmarks:\n",
    "                    cx, cy = int(lm.x * w), int(lm.y * h)\n",
    "                    cv2.circle(frame, (cx, cy), 5, (0, 0, 255), -1)\n",
    "\n",
    "                hand_label = result.handedness[hand_idx][0].category_name\n",
    "\n",
    "                # # 3 Apply gesture rules\n",
    "                for name, rule_fn, action_fn in GESTURE_RULES:\n",
    "                    if rule_fn(landmarks):\n",
    "                        action_fn(hand_label)\n",
    "\n",
    "        # Display the camera feed\n",
    "        cv2.imshow(\"MediaPipe Hands\", frame)\n",
    "\n",
    "        # Exit on 'q'\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "finally:\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    print(\"Camera closed cleanly\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
